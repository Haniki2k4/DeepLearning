{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq6JowrRfeA6"
      },
      "source": [
        "# Using Resnet for chest X-ray Tuberculosis classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjNPJCQs10Oe",
        "outputId": "82c83790-2eac-4a68-a37e-c98b058cac7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\miniconda3\\lib\\site-packages (from opendatasets) (4.66.4)\n",
            "Collecting kaggle (from opendatasets)\n",
            "  Downloading kaggle-1.7.4.5-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: click in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from opendatasets) (8.2.1)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\miniconda3\\lib\\site-packages (from click->opendatasets) (0.4.6)\n",
            "Requirement already satisfied: bleach in c:\\programdata\\miniconda3\\lib\\site-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in c:\\programdata\\miniconda3\\lib\\site-packages (from kaggle->opendatasets) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer in c:\\programdata\\miniconda3\\lib\\site-packages (from kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna in c:\\programdata\\miniconda3\\lib\\site-packages (from kaggle->opendatasets) (3.7)\n",
            "Requirement already satisfied: protobuf in c:\\programdata\\miniconda3\\lib\\site-packages (from kaggle->opendatasets) (6.30.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\programdata\\miniconda3\\lib\\site-packages (from kaggle->opendatasets) (2.9.0.post0)\n",
            "Collecting python-slugify (from kaggle->opendatasets)\n",
            "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: requests in c:\\programdata\\miniconda3\\lib\\site-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python312\\site-packages (from kaggle->opendatasets) (80.9.0)\n",
            "Requirement already satisfied: six>=1.10 in c:\\programdata\\miniconda3\\lib\\site-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Collecting text-unidecode (from kaggle->opendatasets)\n",
            "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in c:\\programdata\\miniconda3\\lib\\site-packages (from kaggle->opendatasets) (2.2.2)\n",
            "Requirement already satisfied: webencodings in c:\\programdata\\miniconda3\\lib\\site-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Downloading kaggle-1.7.4.5-py3-none-any.whl (181 kB)\n",
            "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
            "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
            "Installing collected packages: text-unidecode, python-slugify, kaggle, opendatasets\n",
            "\n",
            "   ---------- ----------------------------- 1/4 [python-slugify]\n",
            "   ---------- ----------------------------- 1/4 [python-slugify]\n",
            "   ---------- ----------------------------- 1/4 [python-slugify]\n",
            "   ---------- ----------------------------- 1/4 [python-slugify]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   -------------------- ------------------- 2/4 [kaggle]\n",
            "   ------------------------------ --------- 3/4 [opendatasets]\n",
            "   ------------------------------ --------- 3/4 [opendatasets]\n",
            "   ------------------------------ --------- 3/4 [opendatasets]\n",
            "   ---------------------------------------- 4/4 [opendatasets]\n",
            "\n",
            "Successfully installed kaggle-1.7.4.5 opendatasets-0.1.22 python-slugify-8.0.4 text-unidecode-1.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script slugify.exe is installed in 'C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script kaggle.exe is installed in 'C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        }
      ],
      "source": [
        "pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf_sNoXv1CH0",
        "outputId": "c559c0c7-b5b7-45d4-dbf4-997a68a78312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle username:Your Kaggle Key:Dataset URL: https://www.kaggle.com/datasets/tawsifurrahman/tuberculosis-tb-chest-xray-dataset\n",
            "Downloading tuberculosis-tb-chest-xray-dataset.zip to .\\tuberculosis-tb-chest-xray-dataset\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 663M/663M [00:00<00:00, 736MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import opendatasets as od\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download the dataset\n",
        "dataset_url = 'https://www.kaggle.com/datasets/tawsifurrahman/tuberculosis-tb-chest-xray-dataset'\n",
        "od.download(dataset_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K3yX_PIkCJSb"
      },
      "outputs": [],
      "source": [
        "# Define the data directory\n",
        "data_dir = r'C:\\Users\\Admin\\Downloads\\DeepLearning\\Practice\\w3.1\\tuberculosis-tb-chest-xray-dataset\\TB_Chest_Radiography_Database'\n",
        "# Define model checkpoints directory\n",
        "save_dir = \"checkpoints\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mud_PJfqBFq8"
      },
      "outputs": [],
      "source": [
        "# ---------------------------\n",
        "# Reproducibility & device\n",
        "# ---------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ---------------------------\n",
        "# Utility: metrics\n",
        "# ---------------------------\n",
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "    y_true, y_probs, y_pred = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            logits = model(xb).squeeze(-1).cpu()  # [batch]\n",
        "            probs = torch.sigmoid(logits).numpy()\n",
        "            preds = (probs >= 0.5).astype(int)\n",
        "            y_probs.extend(probs.tolist())\n",
        "            y_pred.extend(preds.tolist())\n",
        "            y_true.extend(yb.numpy().tolist())\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_probs)\n",
        "    except Exception:\n",
        "        auc = float(\"nan\")\n",
        "    cls_report = classification_report(y_true, y_pred, digits=4)\n",
        "    return acc, auc, cls_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V8mEz0VkBKL0"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "\n",
        "# transforms\n",
        "tf = transforms.Compose([\n",
        "      transforms.RandomResizedCrop((224,224)),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "  ])\n",
        "\n",
        "full_ds = datasets.ImageFolder(os.path.join(data_dir), transform=tf)\n",
        "# Store class_to_idx before splitting\n",
        "class_to_idx = full_ds.class_to_idx\n",
        "\n",
        "train_len = int(len(full_ds)*0.8)\n",
        "val_len = len(full_ds) - train_len\n",
        "train_ds, val_ds = random_split(full_ds, [train_len, val_len], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=1)\n",
        "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w5ohgv3fUQY",
        "outputId": "ba5e98ac-fa8a-437f-b1e6-a94d90261e1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 210/210 [23:03<00:00,  6.59s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 0] train_loss=0.2791 val_acc=0.8107 val_auc=0.9476 \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9837    0.7835    0.8723       693\n",
            "           1     0.4792    0.9388    0.6345       147\n",
            "\n",
            "    accuracy                         0.8107       840\n",
            "   macro avg     0.7314    0.8612    0.7534       840\n",
            "weighted avg     0.8954    0.8107    0.8307       840\n",
            "\n",
            "  Saved best checkpoint to checkpoints\\tb_resnet50_best.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 210/210 [2:33:10<00:00, 43.76s/it]     \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] train_loss=0.2133 val_acc=0.8702 val_auc=0.9460 \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9725    0.8672    0.9169       693\n",
            "           1     0.5856    0.8844    0.7046       147\n",
            "\n",
            "    accuracy                         0.8702       840\n",
            "   macro avg     0.7790    0.8758    0.8107       840\n",
            "weighted avg     0.9048    0.8702    0.8797       840\n",
            "\n",
            "Final val acc=0.8655, auc=0.9452 \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9739    0.8600    0.9134       693\n",
            "           1     0.5746    0.8912    0.6987       147\n",
            "\n",
            "    accuracy                         0.8655       840\n",
            "   macro avg     0.7742    0.8756    0.8060       840\n",
            "weighted avg     0.9040    0.8655    0.8758       840\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# model:resnet50 -> single logit output\n",
        "# using pretrained with weights='DEFAULT'\n",
        "# training from scratch with weights=None\n",
        "model = models.resnet50(weights=None)\n",
        "\n",
        "for param in model.parameters():\n",
        "      param.requires_grad = True  # fine-tune all (or set False to freeze)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 1)  # single logit\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2)\n",
        "\n",
        "best_auc = 0.0\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "for epoch in range(2):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in tqdm(train_loader):\n",
        "        xb, yb = xb.to(device), yb.float().to(device)\n",
        "        logits = model(xb).squeeze(-1)  # [batch]\n",
        "        loss = criterion(logits, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    train_loss = running_loss / len(train_loader.dataset)\n",
        "    val_acc, val_auc, val_report = evaluate_model(model, val_loader)\n",
        "    scheduler.step(val_loss := train_loss)  # or use val_auc etc\n",
        "    print(f\"[Epoch {epoch}] train_loss={train_loss:.4f} val_acc={val_acc:.4f} val_auc={val_auc:.4f} \\n {val_report}\")\n",
        "\n",
        "    if val_auc > best_auc:\n",
        "        best_auc = val_auc\n",
        "        ckpt = os.path.join(save_dir, \"tb_resnet50_best.pt\")\n",
        "        torch.save({\"model_state\": model.state_dict(), \"class_to_idx\": class_to_idx}, ckpt)\n",
        "        print(f\"  Saved best checkpoint to {ckpt}\")\n",
        "\n",
        "  # final eval\n",
        "test_acc, test_auc, test_report = evaluate_model(model, val_loader)\n",
        "print(f\"Final val acc={test_acc:.4f}, auc={test_auc:.4f} \\n {test_report}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
